{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jz3502/.local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:471: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/jz3502/.local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:472: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/jz3502/.local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:473: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/jz3502/.local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:474: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/jz3502/.local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:475: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import torch \n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset, SequentialSampler\n",
    "import tqdm\n",
    "from transformers import BertForSequenceClassification, AdamW, BertTokenizer, BertModel\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('train_1k.csv', index_col = 0)\n",
    "val = pd.read_csv('val_1k.csv', index_col = 0)\n",
    "test = pd.read_csv('test_1k.csv', index_col = 0)\n",
    "\n",
    "train.dropna(subset = ['ICD9_CODE_1k'], inplace = True)\n",
    "val.dropna(subset = ['ICD9_CODE_1k'], inplace = True)\n",
    "test.dropna(subset = ['ICD9_CODE_1k'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Get all labels\n",
    "\n",
    "label_list = []\n",
    "for code in train['ICD9_CODE_1k']:\n",
    "    labels = code.split(',')\n",
    "    label_list.extend([label for label in labels if label not in label_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "label_dict = {}\n",
    "for i, label in enumerate(label_list):\n",
    "    label_dict[label] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "bert = BertModel.from_pretrained('bert-base-cased', output_attentions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "bert.embeddings.word_embeddings = nn.Embedding(tokenizer.vocab_size, 768, padding_idx = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Split the sequence in half and then tokenize seperately\n",
    "\n",
    "def bert_tokenize(data, max_length, label_dict):\n",
    "    \n",
    "    input_ids_first = []\n",
    "    input_ids_second = []\n",
    "    attention_masks_first = []\n",
    "    attention_masks_second = []\n",
    "    labels = []\n",
    "    \n",
    "    for sentence in data['TEXT']:\n",
    "        text_len = len(sentence)\n",
    "\n",
    "        encoded_dict_first = tokenizer.encode_plus(sentence[:text_len//2], add_special_tokens = True, max_length = max_length,\\\n",
    "                                             pad_to_max_length = True, return_attention_mask = True, return_tensors = 'pt')\n",
    "        input_ids_first.append(encoded_dict_first['input_ids'])\n",
    "        attention_masks_first.append(encoded_dict_first['attention_mask'])\n",
    "   \n",
    "        \n",
    "        encoded_dict_second = tokenizer.encode_plus(sentence[text_len//2:], add_special_tokens = True, max_length = max_length,\\\n",
    "                                             pad_to_max_length = True, return_attention_mask = True, return_tensors = 'pt')\n",
    "        input_ids_second.append(encoded_dict_second['input_ids'])\n",
    "        attention_masks_second.append(encoded_dict_second['attention_mask'])\n",
    "        \n",
    "    for codes in data['ICD9_CODE_1k']:\n",
    "        label = [0]*1000\n",
    "        \n",
    "        all_code = codes.split(',')\n",
    "        for code in all_code:\n",
    "            label[label_dict[code]] = 1\n",
    "        \n",
    "        labels.append(torch.tensor(label))\n",
    "\n",
    "\n",
    "        \n",
    "    input_ids_first = torch.cat(input_ids_first, dim=0)\n",
    "    attention_masks_first = torch.cat(attention_masks_first, dim=0)\n",
    "    input_ids_second = torch.cat(input_ids_second, dim=0)\n",
    "    attention_masks_second = torch.cat(attention_masks_second, dim=0)\n",
    "    labels = torch.stack(labels, dim=0)\n",
    "        \n",
    "    return input_ids_first, attention_masks_first, input_ids_second, attention_masks_second, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "\n",
    "input_ids_first_train, attention_masks_first_train, input_ids_second_train,\\\n",
    "attention_masks_second_train, labels_train = bert_tokenize(train, 512, label_dict)\n",
    "\n",
    "train_dataset_bert = TensorDataset(input_ids_first_train, attention_masks_first_train, input_ids_second_train,\\\n",
    "                                   attention_masks_second_train, labels_train)\n",
    "train_loader_bert = DataLoader(train_dataset_bert, shuffle = True, batch_size = batch_size)\n",
    "\n",
    "input_ids_first_val, attention_masks_first_val, input_ids_second_val,\\\n",
    "attention_masks_second_val, labels_val = bert_tokenize(val, 512, label_dict)\n",
    "\n",
    "val_dataset_bert = TensorDataset(input_ids_first_val, attention_masks_first_val, input_ids_second_val,\\\n",
    "                                 attention_masks_second_val, labels_val)\n",
    "val_loader_bert = DataLoader(val_dataset_bert, shuffle = True, batch_size = batch_size)\n",
    "\n",
    "input_ids_first_test, attention_masks_first_test, input_ids_second_test,\\\n",
    "attention_masks_second_test, labels_test = bert_tokenize(test, 512, label_dict)\n",
    "\n",
    "test_dataset_bert = TensorDataset(input_ids_first_test, attention_masks_first_test, input_ids_second_test,\\\n",
    "                                  attention_masks_second_test, labels_test)\n",
    "test_sampler_bert = SequentialSampler(test_dataset_bert)\n",
    "test_loader_bert = DataLoader(test_dataset_bert, sampler = test_sampler_bert, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# torch.save(train_loader_bert, 'train_dataloader.pth')\n",
    "# torch.save(val_loader_bert, 'val_dataloader.pth')\n",
    "# torch.save(test_loader_bert, 'test_dataloader.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_loader_bert = torch.load('train_dataloader.pth')\n",
    "val_loader_bert = torch.load('val_dataloader.pth')\n",
    "test_loader_bert = torch.load('test_dataloader.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Train first and second half of the sequence seperately, then concatenate the hidden state output\n",
    "\n",
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self, bert, num_classes):\n",
    "        super().__init__()\n",
    "        self.bert = bert\n",
    "        self.linear = nn.Linear(bert.config.hidden_size*2, num_classes)\n",
    "        self.num_classes = num_classes\n",
    "    \n",
    "    def forward(self, input_ids_first, attention_masks_first, input_ids_second, attention_masks_second):\n",
    "        h1, _, _ = self.bert(input_ids = input_ids_first, attention_mask = attention_masks_first)\n",
    "        h1_cls = h1[:, 0]\n",
    "        h2, _, _ = self.bert(input_ids = input_ids_second, attention_mask = attention_masks_second)\n",
    "        h2_cls = h2[:, 0]\n",
    "        h_cls = torch.cat((h1_cls, h2_cls), dim = -1)\n",
    "        logits = self.linear(h_cls)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model_bert = BERTClassifier(bert, 1000).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss(reduction = 'sum')\n",
    "optimizer_bert = AdamW(model_bert.parameters(), lr = 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current epoch is 0\n"
     ]
    }
   ],
   "source": [
    "train_loss_list_bert = []\n",
    "val_loss_list_bert = []\n",
    "\n",
    "for epoch in range(5):\n",
    "    print(\"current epoch is \"+str(epoch))\n",
    "    train_loss = 0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    val_loss = 0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    model_bert.train()\n",
    "    for i, (input_ids_first, attention_masks_first, input_ids_second,\n",
    "            attention_masks_second, labels) in enumerate(train_loader_bert):\n",
    "        \n",
    "        optimizer_bert.zero_grad()\n",
    "        input_ids_first = input_ids_first.to(device)\n",
    "        attention_masks_first = attention_masks_first.to(device)\n",
    "        input_ids_second = input_ids_second.to(device)\n",
    "        attention_masks_second = attention_masks_second.to(device)\n",
    "        labels = labels.to(device).float()\n",
    "\n",
    "        \n",
    "        \n",
    "        logits = model_bert(input_ids_first, attention_masks_first, input_ids_second, attention_masks_second)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer_bert.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        train_total += input_ids_first.size()[0]\n",
    "        \n",
    "    train_avg_loss = train_loss / train_total   \n",
    "    train_loss_list_bert.append(train_avg_loss)\n",
    "    \n",
    "    model_bert.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, (input_ids_first, attention_masks_first, input_ids_second,\n",
    "                attention_masks_second, labels) in enumerate(val_loader_bert):\n",
    "        \n",
    "            input_ids_first = input_ids_first.to(device)\n",
    "            attention_masks_first = attention_masks_first.to(device)\n",
    "            input_ids_second = input_ids_second.to(device)\n",
    "            attention_masks_second = attention_masks_second.to(device)\n",
    "            labels = labels.to(device).float()\n",
    "\n",
    "            logits = model_bert(input_ids_first, attention_masks_first, input_ids_second, attention_masks_second)\n",
    "            loss = criterion(logits, labels)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            val_total += input_ids_first.size()[0]       \n",
    "        \n",
    "\n",
    "    val_avg_loss = val_loss / val_total  \n",
    "    val_loss_list_bert.append(val_avg_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "plt.plot(np.arange(5), train_loss_list_bert, label = 'train')\n",
    "plt.plot(np.arange(5), val_loss_list_bert, label = 'validation')\n",
    "plt.legend()\n",
    "plt.title('Bert Average Loss over Epoch')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Average Loss')\n",
    "plt.savefig('loss.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# torch.save(model_bert, 'model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model_bert = torch.load('model.pt').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "test_label = []\n",
    "test_logits = []\n",
    "test_prediction = []\n",
    "\n",
    "model_bert.eval()\n",
    "with torch.no_grad():\n",
    "    for i, (input_ids_first, attention_masks_first, input_ids_second,\n",
    "            attention_masks_second, labels) in enumerate(test_loader_bert):\n",
    "\n",
    "        input_ids_first = input_ids_first.to(device)\n",
    "        attention_masks_first = attention_masks_first.to(device)\n",
    "        input_ids_second = input_ids_second.to(device)\n",
    "        attention_masks_second = attention_masks_second.to(device)\n",
    "        labels = labels.to(device).float()\n",
    "\n",
    "        logits = model_bert(input_ids_first, attention_masks_first, input_ids_second, attention_masks_second)\n",
    "        sigmoid_logits = torch.sigmoid(logits)\n",
    "        prediction = torch.where(sigmoid_logits > 0.5, torch.tensor(1).to(device), torch.tensor(0).to(device))\n",
    "        \n",
    "        test_label.extend(labels.tolist())\n",
    "        test_logits.extend(logits.tolist())\n",
    "        test_prediction.extend(prediction.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "test_label_array = np.array(test_label)\n",
    "test_logits_array = np.array(test_logits)\n",
    "test_prediction_array = np.array(test_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Show precision and recall (at 10 and at 5)\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# At 10\n",
    "\n",
    "label_at_10 = []\n",
    "prediction_at_10 = []\n",
    "\n",
    "top_10 = test_logits_array.argsort(axis = 1)[:,-10:]\n",
    "for row,top in enumerate(top_10):\n",
    "    test_label_top = test_label_array[row][top]\n",
    "    test_prediction_top = test_prediction_array[row][top]\n",
    "\n",
    "    label_at_10.extend(test_label_top)\n",
    "    prediction_at_10.extend(test_prediction_top)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "precision_at_10 = precision_score(label_at_10, prediction_at_10)\n",
    "recall_at_10 = recall_score(label_at_10, prediction_at_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7183430418737992\n",
      "0.5064995806722147\n"
     ]
    }
   ],
   "source": [
    "print(precision_at_10)\n",
    "print(recall_at_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# At 5\n",
    "\n",
    "label_at_5 = []\n",
    "prediction_at_5 = []\n",
    "\n",
    "top_5 = test_logits_array.argsort(axis = 1)[:,-5:]\n",
    "for row,top in enumerate(top_5):\n",
    "    test_label_top = test_label_array[row][top]\n",
    "    test_prediction_top = test_prediction_array[row][top]\n",
    "\n",
    "    label_at_5.extend(test_label_top)\n",
    "    prediction_at_5.extend(test_prediction_top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "precision_at_5 = precision_score(label_at_5, prediction_at_5)\n",
    "recall_at_5 = recall_score(label_at_5, prediction_at_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7276980053277889\n",
      "0.7065688825802382\n"
     ]
    }
   ],
   "source": [
    "print(precision_at_5)\n",
    "print(recall_at_5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
